{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9854243,"sourceType":"datasetVersion","datasetId":6047067}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport timm\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nimport numpy as np\nfrom tqdm import tqdm\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport json\nfrom datetime import datetime\n\nclass MedicalImageDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        image = Image.open(self.image_paths[idx]).convert('RGB')\n        label = self.labels[idx]\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label\n\nclass MedicalClassifier:\n    def __init__(self, num_classes=3, model_name='efficientnetv2_s', device='cuda'):\n        self.device = device\n        self.num_classes = num_classes\n        self.model_name = model_name\n        self.best_val_acc = 0.0\n        self.train_losses = []\n        self.val_losses = []\n        self.train_accuracies = []\n        self.val_accuracies = []\n        \n        # Set directories for saving models and logs (use Kaggle's output directory)\n        self.checkpoint_dir = '/kaggle/working/checkpoints'\n        self.logs_dir = '/kaggle/working/logs'\n        os.makedirs(self.checkpoint_dir, exist_ok=True)\n        os.makedirs(self.logs_dir, exist_ok=True)\n        \n        # Initialize model\n        self.model = self._initialize_model()\n        self.model = self.model.to(self.device)\n        \n        # Define transforms\n        # Progressive resizing: Start with smaller size, increase during training\n        self.transform_stage1 = transforms.Compose([\n            transforms.Resize((128, 128)),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        \n        self.transform_stage2 = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        \n        self.val_transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n    \n    def _initialize_model(self):\n        model = timm.create_model(self.model_name, pretrained=False, num_classes=self.num_classes)\n        return model\n    \n    def load_data(self, data_dir):\n        \"\"\"Load and split data from directory structure data_dir/class/image.jpg\"\"\"\n        image_paths = []\n        labels = []\n        class_to_idx = {}\n        \n        for idx, class_name in enumerate(os.listdir(data_dir)):\n            class_dir = os.path.join(data_dir, class_name)\n            class_to_idx[class_name] = idx\n            \n            if os.path.isdir(class_dir):\n                for img_name in os.listdir(class_dir):\n                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n                        image_paths.append(os.path.join(class_dir, img_name))\n                        labels.append(idx)\n        \n        # Save class mapping\n        with open(os.path.join(self.logs_dir, 'class_mapping.json'), 'w') as f:\n            json.dump(class_to_idx, f)\n        \n        # Split data\n        train_paths, val_paths, train_labels, val_labels = train_test_split(\n            image_paths, labels, test_size=0.2, random_state=42, stratify=labels\n        )\n        \n        val_paths, test_paths, val_labels, test_labels = train_test_split(\n            val_paths, val_labels, test_size=0.5, random_state=42, stratify=val_labels\n        )\n        \n        # Create datasets\n        train_dataset = MedicalImageDataset(train_paths, train_labels, self.transform_stage1)\n        val_dataset = MedicalImageDataset(val_paths, val_labels, self.val_transform)\n        test_dataset = MedicalImageDataset(test_paths, test_labels, self.val_transform)\n        \n        return train_dataset, val_dataset, test_dataset\n    \n    def train(self, train_dataset, val_dataset, batch_size=32, num_epochs_stage1=20, num_epochs_stage2=30,\n              learning_rate=1e-4, resume_training=False):\n        \"\"\"Train the model with checkpointing and progressive resizing\"\"\"\n        # Stage 1: Train with smaller images (128x128)\n        train_loader_stage1 = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, factor=0.1)\n        \n        # Load previous checkpoint if resuming\n        start_epoch = 0\n        if resume_training:\n            checkpoint_path = os.path.join(self.checkpoint_dir, 'latest_checkpoint.pth')\n            if os.path.exists(checkpoint_path):\n                checkpoint = torch.load(checkpoint_path)\n                self.model.load_state_dict(checkpoint['model_state_dict'])\n                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n                start_epoch = checkpoint['epoch'] + 1\n                self.best_val_acc = checkpoint['best_val_acc']\n                print(f\"Resuming training from epoch {start_epoch}\")\n        \n        # Training loop\n        for epoch in range(start_epoch, num_epochs_stage1):\n            self.model.train()\n            train_loss = 0\n            correct = 0\n            total = 0\n            \n            for images, labels in tqdm(train_loader_stage1, desc=f'Epoch {epoch+1}/{num_epochs_stage1} [Train Stage 1]'):\n                images, labels = images.to(self.device), labels.to(self.device)\n                \n                optimizer.zero_grad()\n                outputs = self.model(images)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n                \n                train_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n            \n            epoch_train_loss = train_loss / len(train_loader_stage1)\n            epoch_train_acc = 100. * correct / total\n            \n            scheduler.step(epoch_train_acc)  # Adjust learning rate\n        \n        # Stage 2: Train with higher-resolution images (224x224)\n        train_dataset.transform = self.transform_stage2\n        train_loader_stage2 = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        \n        for epoch in range(num_epochs_stage1, num_epochs_stage1 + num_epochs_stage2):\n            self.model.train()\n            train_loss = 0\n            correct = 0\n            total = 0\n            \n            for images, labels in tqdm(train_loader_stage2, desc=f'Epoch {epoch+1}/{num_epochs_stage2} [Train Stage 2]'):\n                images, labels = images.to(self.device), labels.to(self.device)\n                \n                optimizer.zero_grad()\n                outputs = self.model(images)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n                \n                train_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n            \n            epoch_train_loss = train_loss / len(train_loader_stage2)\n            epoch_train_acc = 100. * correct / total\n            \n            print(f'Epoch {epoch+1}/{num_epochs_stage1} - Loss: {epoch_train_loss:.4f}, Accuracy: {epoch_train_acc:.2f}%')\n            \n            scheduler.step(epoch_train_acc)  # Adjust learning rate\n                \n\n        # Save best model\n        torch.save(self.model.state_dict(), os.path.join(self.checkpoint_dir, 'best_model.pth'))\n    \n    def evaluate(self, test_dataset, batch_size=32):\n        \"\"\"Evaluate the model on test set\"\"\"\n        self.model.eval()\n        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n        \n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for images, labels in tqdm(test_loader, desc='Testing'):\n                images, labels = images.to(self.device), labels.to(self.device)\n                outputs = self.model(images)\n                _, predicted = outputs.max(1)\n                \n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n        \n        accuracy = 100. * correct / total\n        print(f'Test Accuracy: {accuracy:.2f}%')\n        \n        return accuracy\n\n# Usage example\nif __name__ == \"__main__\":\n    classifier = MedicalClassifier(num_classes=3, model_name='efficientnetv2_s',\n                                   device='cuda' if torch.cuda.is_available() else 'cpu')\n    \n    data_dir = '/kaggle/input/medical-image-classification/dataset2'  # Update with your Kaggle dataset path\n    train_dataset, val_dataset, test_dataset = classifier.load_data(data_dir)\n    \n    classifier.train(train_dataset, val_dataset, batch_size=32, num_epochs_stage1=10, num_epochs_stage2=15)\n    classifier.evaluate(test_dataset)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-09T19:00:50.399420Z","iopub.execute_input":"2024-11-09T19:00:50.399816Z","iopub.status.idle":"2024-11-09T20:18:40.231661Z","shell.execute_reply.started":"2024-11-09T19:00:50.399778Z","shell.execute_reply":"2024-11-09T20:18:40.230491Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Epoch 1/10 [Train Stage 1]: 100%|██████████| 979/979 [02:21<00:00,  6.91it/s]\nEpoch 2/10 [Train Stage 1]: 100%|██████████| 979/979 [02:03<00:00,  7.91it/s]\nEpoch 3/10 [Train Stage 1]: 100%|██████████| 979/979 [02:04<00:00,  7.87it/s]\nEpoch 4/10 [Train Stage 1]: 100%|██████████| 979/979 [02:04<00:00,  7.88it/s]\nEpoch 5/10 [Train Stage 1]: 100%|██████████| 979/979 [02:03<00:00,  7.92it/s]\nEpoch 6/10 [Train Stage 1]: 100%|██████████| 979/979 [02:02<00:00,  7.97it/s]\nEpoch 7/10 [Train Stage 1]: 100%|██████████| 979/979 [02:01<00:00,  8.08it/s]\nEpoch 8/10 [Train Stage 1]: 100%|██████████| 979/979 [02:03<00:00,  7.93it/s]\nEpoch 9/10 [Train Stage 1]: 100%|██████████| 979/979 [02:01<00:00,  8.04it/s]\nEpoch 10/10 [Train Stage 1]: 100%|██████████| 979/979 [02:01<00:00,  8.05it/s]\nEpoch 11/15 [Train Stage 2]: 100%|██████████| 979/979 [03:46<00:00,  4.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/10 - Loss: 0.0643, Accuracy: 98.04%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/15 [Train Stage 2]: 100%|██████████| 979/979 [03:46<00:00,  4.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/10 - Loss: 0.0091, Accuracy: 99.65%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/15 [Train Stage 2]: 100%|██████████| 979/979 [03:46<00:00,  4.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/10 - Loss: 0.0137, Accuracy: 99.58%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/15 [Train Stage 2]: 100%|██████████| 979/979 [03:46<00:00,  4.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/10 - Loss: 0.0093, Accuracy: 99.70%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/15 [Train Stage 2]: 100%|██████████| 979/979 [03:46<00:00,  4.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/10 - Loss: 0.0091, Accuracy: 99.74%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/15 [Train Stage 2]: 100%|██████████| 979/979 [03:46<00:00,  4.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/10 - Loss: 0.0304, Accuracy: 99.54%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/15 [Train Stage 2]: 100%|██████████| 979/979 [03:46<00:00,  4.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/10 - Loss: 0.0162, Accuracy: 99.52%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/15 [Train Stage 2]: 100%|██████████| 979/979 [03:46<00:00,  4.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/10 - Loss: 0.0104, Accuracy: 99.81%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/15 [Train Stage 2]: 100%|██████████| 979/979 [03:46<00:00,  4.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/10 - Loss: 0.0116, Accuracy: 99.62%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/15 [Train Stage 2]: 100%|██████████| 979/979 [03:46<00:00,  4.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20/10 - Loss: 0.0029, Accuracy: 99.90%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/15 [Train Stage 2]: 100%|██████████| 979/979 [03:46<00:00,  4.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21/10 - Loss: 0.0061, Accuracy: 99.82%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/15 [Train Stage 2]: 100%|██████████| 979/979 [03:46<00:00,  4.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22/10 - Loss: 0.0057, Accuracy: 99.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/15 [Train Stage 2]: 100%|██████████| 979/979 [03:47<00:00,  4.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23/10 - Loss: 0.0049, Accuracy: 99.86%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/15 [Train Stage 2]: 100%|██████████| 979/979 [03:46<00:00,  4.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24/10 - Loss: 0.0040, Accuracy: 99.89%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/15 [Train Stage 2]: 100%|██████████| 979/979 [03:46<00:00,  4.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25/10 - Loss: 0.0224, Accuracy: 99.71%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 123/123 [00:19<00:00,  6.47it/s]","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 100.00%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-11-09T21:23:18.051820Z","iopub.execute_input":"2024-11-09T21:23:18.052697Z","iopub.status.idle":"2024-11-09T21:23:18.065339Z","shell.execute_reply.started":"2024-11-09T21:23:18.052654Z","shell.execute_reply":"2024-11-09T21:23:18.064307Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-11-09T21:46:44.764351Z","iopub.execute_input":"2024-11-09T21:46:44.765214Z","iopub.status.idle":"2024-11-09T21:46:55.048304Z","shell.execute_reply.started":"2024-11-09T21:46:44.765167Z","shell.execute_reply":"2024-11-09T21:46:55.047263Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Testing: 100%|██████████| 123/123 [00:10<00:00, 12.17it/s]","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 100.00%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-11-09T21:49:11.760610Z","iopub.execute_input":"2024-11-09T21:49:11.761016Z","iopub.status.idle":"2024-11-09T21:49:12.023672Z","shell.execute_reply.started":"2024-11-09T21:49:11.760977Z","shell.execute_reply":"2024-11-09T21:49:12.022616Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-11-09T21:49:29.951272Z","iopub.execute_input":"2024-11-09T21:49:29.951653Z","iopub.status.idle":"2024-11-09T21:49:29.959043Z","shell.execute_reply.started":"2024-11-09T21:49:29.951617Z","shell.execute_reply":"2024-11-09T21:49:29.958050Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/checkpoints/best_model.pth","text/html":"<a href='/kaggle/working/checkpoints/best_model.pth' target='_blank'>/kaggle/working/checkpoints/best_model.pth</a><br>"},"metadata":{}}]}]}